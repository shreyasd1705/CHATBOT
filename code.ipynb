{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8166041,"sourceType":"datasetVersion","datasetId":4832118}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-o1nly \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-20T19:57:10.638329Z","iopub.execute_input":"2024-04-20T19:57:10.638975Z","iopub.status.idle":"2024-04-20T19:57:11.520053Z","shell.execute_reply.started":"2024-04-20T19:57:10.638943Z","shell.execute_reply":"2024-04-20T19:57:11.519058Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/dataset1/preprocessed.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import openpyxl\nimport torch\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-04-20T19:58:35.439907Z","iopub.execute_input":"2024-04-20T19:58:35.440423Z","iopub.status.idle":"2024-04-20T19:58:35.445724Z","shell.execute_reply.started":"2024-04-20T19:58:35.440392Z","shell.execute_reply":"2024-04-20T19:58:35.444738Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"workbook = openpyxl.Workbook()\nworksheet = workbook.active\nworksheet.append([\"Fold\", \"Model\", \"Test Accuracy Before\", \"Test F1 Before\", \"Test MCC Before\",\n                  \"Test Accuracy After\", \"Test F1 After\", \"Test MCC After\"])","metadata":{"execution":{"iopub.status.busy":"2024-04-20T19:58:35.887252Z","iopub.execute_input":"2024-04-20T19:58:35.887616Z","iopub.status.idle":"2024-04-20T19:58:35.894631Z","shell.execute_reply.started":"2024-04-20T19:58:35.887587Z","shell.execute_reply":"2024-04-20T19:58:35.893698Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"dataset=pd.read_csv(\"/kaggle/input/dataset1/preprocessed.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-04-20T19:58:36.271785Z","iopub.execute_input":"2024-04-20T19:58:36.272104Z","iopub.status.idle":"2024-04-20T19:58:36.327917Z","shell.execute_reply.started":"2024-04-20T19:58:36.272080Z","shell.execute_reply":"2024-04-20T19:58:36.327062Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"dataset.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T19:58:36.587670Z","iopub.execute_input":"2024-04-20T19:58:36.588007Z","iopub.status.idle":"2024-04-20T19:58:36.608565Z","shell.execute_reply.started":"2024-04-20T19:58:36.587982Z","shell.execute_reply":"2024-04-20T19:58:36.607687Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"             tweet_id                                              tweet  \\\n0  866871160725794816          Triple Talaq par Burbak Kuchh nahi bolega   \n1  880356789358743553  Batao ye uss site pr se akki sir ke verdict ni...   \n2  877751493889105920  Hindu baheno par julam bardas nahi hoga @Tripl...   \n3  901806457871466496  Naa bhai.. aisa nhi hai.. mere handle karne se...   \n4  866264330748219392  #RememberingRajiv aaj agar musalman auraten tr...   \n\n   sarcasm                                   processed_tweets  \n0        0          Triple Talaq par Burbak Kuchh nahi bolega  \n1        1  Batao ye uss site pr se akki sir ke verdict ni...  \n2        0  Hindu baheno par julam bardas nahi hoga Hindu ...  \n3        0  Naa bhai .. aisa nhi hai .. mere handle karne ...  \n4        0  Remembering Rajiv aaj agar musalman auraten tr...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>tweet</th>\n      <th>sarcasm</th>\n      <th>processed_tweets</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>866871160725794816</td>\n      <td>Triple Talaq par Burbak Kuchh nahi bolega</td>\n      <td>0</td>\n      <td>Triple Talaq par Burbak Kuchh nahi bolega</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>880356789358743553</td>\n      <td>Batao ye uss site pr se akki sir ke verdict ni...</td>\n      <td>1</td>\n      <td>Batao ye uss site pr se akki sir ke verdict ni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>877751493889105920</td>\n      <td>Hindu baheno par julam bardas nahi hoga @Tripl...</td>\n      <td>0</td>\n      <td>Hindu baheno par julam bardas nahi hoga Hindu ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>901806457871466496</td>\n      <td>Naa bhai.. aisa nhi hai.. mere handle karne se...</td>\n      <td>0</td>\n      <td>Naa bhai .. aisa nhi hai .. mere handle karne ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>866264330748219392</td>\n      <td>#RememberingRajiv aaj agar musalman auraten tr...</td>\n      <td>0</td>\n      <td>Remembering Rajiv aaj agar musalman auraten tr...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dataset.drop(['tweet'],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T19:58:36.888000Z","iopub.execute_input":"2024-04-20T19:58:36.888902Z","iopub.status.idle":"2024-04-20T19:58:36.898244Z","shell.execute_reply.started":"2024-04-20T19:58:36.888868Z","shell.execute_reply":"2024-04-20T19:58:36.897433Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"dataset['processed_tweets'][0]","metadata":{"execution":{"iopub.status.busy":"2024-04-20T19:58:37.174557Z","iopub.execute_input":"2024-04-20T19:58:37.174902Z","iopub.status.idle":"2024-04-20T19:58:37.181186Z","shell.execute_reply.started":"2024-04-20T19:58:37.174875Z","shell.execute_reply":"2024-04-20T19:58:37.180269Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'Triple Talaq par Burbak Kuchh nahi bolega'"},"metadata":{}}]},{"cell_type":"code","source":"template = \"Categorize the following Hinglish text into one of the predefined groups: sarcastic and non-sarcastic. The text is \\\"////\\\".\"\n\n# Format the tweets into the template\ndataset[\"processed_tweets\"] = [template.replace(\"////\", tweet) for tweet in dataset[\"processed_tweets\"]]","metadata":{"execution":{"iopub.status.busy":"2024-04-20T19:58:37.513193Z","iopub.execute_input":"2024-04-20T19:58:37.513991Z","iopub.status.idle":"2024-04-20T19:58:37.522238Z","shell.execute_reply.started":"2024-04-20T19:58:37.513960Z","shell.execute_reply":"2024-04-20T19:58:37.521376Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"test_models=[\"bert-base-multilingual-cased\",\"distilbert-base-multilingual-cased\",\"xlm-roberta-base\",\"timpal0l/mdeberta-v3-base-squad2\",\"microsoft/Multilingual-MiniLM-L12-H384\"]","metadata":{"execution":{"iopub.status.busy":"2024-04-20T19:58:37.955682Z","iopub.execute_input":"2024-04-20T19:58:37.956352Z","iopub.status.idle":"2024-04-20T19:58:37.960625Z","shell.execute_reply.started":"2024-04-20T19:58:37.956319Z","shell.execute_reply":"2024-04-20T19:58:37.959685Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\n# Define evaluation function\ndef evaluate_model(data_loader, model, device):\n    model.eval()\n    predictions = []\n    true_labels = []\n    with torch.no_grad():\n        for inputs, attention_mask, labels in data_loader:\n            inputs, attention_mask, labels = inputs.to(device), attention_mask.to(device), labels.to(device)\n            outputs = model(inputs, attention_mask=attention_mask)\n            _, predicted = outputs.logits.max(1)\n            predictions.extend(predicted.tolist())\n            true_labels.extend(labels.tolist())\n    accuracy = accuracy_score(true_labels, predictions)\n    f1 = f1_score(true_labels, predictions)\n    mcc = matthews_corrcoef(true_labels, predictions)\n    return accuracy, f1, mcc\n\n# Define training loop\ndef train_model(model, train_loader, optimizer, scheduler, device, num_epochs):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n\n        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n            input_ids, attention_mask, labels = batch \n            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        scheduler.step()\n\n        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader):.4f}\")\n\n        # Save the changes to the Excel file after each epoch\n        workbook.save('finetuned_model_accu_scores2.xlsx')\n\n\n# Tokenize input sequences\nmax_length = 128\n\ndef tokenize_data(data, tokenizer, max_length):\n    tokenized_data = tokenizer(data['processed_tweets'].tolist(), padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n    labels = torch.tensor(data['sarcasm'].tolist())\n    return tokenized_data, labels\n\n# Split the dataset into 4 folds\nnum_folds = 4\nkf = KFold(n_splits=num_folds)\n\n# Initialize lists to store evaluation metrics\ntest_accuracy_before_list = []\ntest_accuracy_after_list = []\ntest_f1_before_list = []\ntest_f1_after_list = []\ntest_mcc_before_list = []\ntest_mcc_after_list = []\n\n# Iterate over models\nfor i in range(0,len(test_models)): # 47th modela\n    try:\n        fold = 0\n        for train_index, test_index in kf.split(dataset):\n            fold += 1\n            train_data_fold, test_data_fold = dataset.iloc[train_index], dataset.iloc[test_index]\n            # Load tokenizer and model\n            model_name = test_models[i]\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\n            model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\n            # Move model to appropriate device\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            model.to(device)\n\n            # Define optimizer and learning rate scheduler\n            optimizer = AdamW(model.parameters(), lr=2e-5)\n            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n\n            # Tokenize and create DataLoader for training and testing data\n            tokenized_train_data, train_labels = tokenize_data(train_data_fold, tokenizer, max_length)\n            tokenized_test_data, test_labels = tokenize_data(test_data_fold, tokenizer, max_length)\n\n            train_dataset = TensorDataset(tokenized_train_data['input_ids'], tokenized_train_data['attention_mask'],\n                                          train_labels)\n            test_dataset = TensorDataset(tokenized_test_data['input_ids'], tokenized_test_data['attention_mask'],\n                                         test_labels)\n\n            train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n            test_loader = DataLoader(test_dataset, batch_size=32)\n\n            # Evaluate the model before training\n            test_accuracy_before, test_f1_before, test_mcc_before = evaluate_model(test_loader, model, device)\n            print(f\"Fold {fold}, Model {model_name}:\",i)\n            print(f\"Test Accuracy (Before fine-tuning): {test_accuracy_before:.2f}%\")\n            print(f\"Test F1 Score (Before fine-tuning): {test_f1_before:.2f}\")\n            print(f\"Test Matthews Correlation Coefficient (Before fine-tuning): {test_mcc_before:.2f}\")\n\n            # Train the model\n            num_epochs = 10\n            train_model(model, train_loader, optimizer, scheduler, device, num_epochs)\n\n            # Evaluate the model after training\n            test_accuracy_after, test_f1_after, test_mcc_after = evaluate_model(test_loader, model, device)\n            print(f\"Test Accuracy (After fine-tuning): {test_accuracy_after:.2f}%\")\n            print(f\"Test F1 Score (After fine-tuning): {test_f1_after:.2f}\")\n            print(f\"Test Matthews Correlation Coefficient (After fine-tuning): {test_mcc_after:.2f}\")\n\n            # Store evaluation metrics\n            test_accuracy_before_list.append(test_accuracy_before)\n            test_accuracy_after_list.append(test_accuracy_after)\n            test_f1_before_list.append(test_f1_before)\n            test_f1_after_list.append(test_f1_after)\n            test_mcc_before_list.append(test_mcc_before)\n            test_mcc_after_list.append(test_mcc_after)\n\n            # Append the evaluation metrics to the Excel worksheet\n            worksheet.append([fold, model_name, test_accuracy_before, test_f1_before, test_mcc_before,\n                              test_accuracy_after, test_f1_after, test_mcc_after])\n\n        print('Training and evaluation of model ', model_name, ' complete!')\n    except Exception as e:\n        print(f\"Error in iteration {i}: {str(e)}\")\n\n# Calculate changes in metrics\naccuracy_change = [(after - before) for before, after in zip(test_accuracy_before_list, test_accuracy_after_list)]\nf1_change = [(after - before) for before, after in zip(test_f1_before_list, test_f1_after_list)]\nmcc_change = [(after - before) for before, after in zip(test_mcc_before_list, test_mcc_after_list)]\n\n# Calculate average scores after each fold\navg_accuracy_after_fold = sum(test_accuracy_after_list) / len(test_accuracy_after_list)\navg_f1_after_fold = sum(test_f1_after_list) / len(test_f1_after_list)\navg_mcc_after_fold = sum(test_mcc_after_list) / len(test_mcc_after_list)\n\n# Print or store the results as per your requirement\nprint(\"Average Accuracy After Each Fold:\", avg_accuracy_after_fold)\nprint(\"Average F1 Score After Each Fold:\", avg_f1_after_fold)\nprint(\"Average Matthews Correlation Coefficient After Each Fold:\", avg_mcc_after_fold)\n\n# Save the evaluation metrics to an Excel file\nworkbook.save('evaluation_metrics2.xlsx')","metadata":{"execution":{"iopub.status.busy":"2024-04-20T19:58:38.401560Z","iopub.execute_input":"2024-04-20T19:58:38.401905Z","iopub.status.idle":"2024-04-20T21:59:10.160460Z","shell.execute_reply.started":"2024-04-20T19:58:38.401880Z","shell.execute_reply":"2024-04-20T21:59:10.159450Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3198674f045e4a9d81f6c44f4e2eb778"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a29443dcae44b28a1ab9aef24f8f37e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8076c18e4024bffab3c16821d626453"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55575c8d20c44560b9f7d7fc92fe7b00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2cd9a52504b49059c997f462e1c6ed7"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 1, Model bert-base-multilingual-cased: 0\nTest Accuracy (Before fine-tuning): 0.87%\nTest F1 Score (Before fine-tuning): 0.01\nTest Matthews Correlation Coefficient (Before fine-tuning): -0.04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 123/123 [00:43<00:00,  2.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.3154\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 123/123 [00:42<00:00,  2.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Loss: 0.1744\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 123/123 [00:42<00:00,  2.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Loss: 0.1220\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 123/123 [00:43<00:00,  2.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Loss: 0.1035\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 123/123 [00:42<00:00,  2.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Loss: 0.0834\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 123/123 [00:43<00:00,  2.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Loss: 0.0561\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 123/123 [00:43<00:00,  2.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Loss: 0.0321\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 123/123 [00:42<00:00,  2.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Loss: 0.0246\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 123/123 [00:43<00:00,  2.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Loss: 0.0140\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 123/123 [00:42<00:00,  2.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Loss: 0.0086\nTest Accuracy (After fine-tuning): 0.96%\nTest F1 Score (After fine-tuning): 0.77\nTest Matthews Correlation Coefficient (After fine-tuning): 0.76\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 2, Model bert-base-multilingual-cased: 0\nTest Accuracy (Before fine-tuning): 0.90%\nTest F1 Score (Before fine-tuning): 0.00\nTest Matthews Correlation Coefficient (Before fine-tuning): 0.00\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 124/124 [00:43<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.2725\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 124/124 [00:43<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Loss: 0.1257\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 124/124 [00:43<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Loss: 0.1077\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 124/124 [00:43<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Loss: 0.0895\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 124/124 [00:43<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Loss: 0.0737\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 124/124 [00:43<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Loss: 0.0568\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 124/124 [00:43<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Loss: 0.0399\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 124/124 [00:43<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Loss: 0.0323\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 124/124 [00:43<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Loss: 0.0186\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 124/124 [00:43<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Loss: 0.0168\nTest Accuracy (After fine-tuning): 0.95%\nTest F1 Score (After fine-tuning): 0.65\nTest Matthews Correlation Coefficient (After fine-tuning): 0.67\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 3, Model bert-base-multilingual-cased: 0\nTest Accuracy (Before fine-tuning): 0.33%\nTest F1 Score (Before fine-tuning): 0.19\nTest Matthews Correlation Coefficient (Before fine-tuning): 0.06\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 124/124 [00:43<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.2474\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 124/124 [00:43<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Loss: 0.1299\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 124/124 [00:43<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Loss: 0.1202\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 124/124 [00:43<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Loss: 0.1071\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 124/124 [00:43<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Loss: 0.0999\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 124/124 [00:43<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Loss: 0.0876\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 124/124 [00:43<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Loss: 0.0721\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 124/124 [00:43<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Loss: 0.0520\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 124/124 [00:43<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Loss: 0.0440\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 124/124 [00:43<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Loss: 0.0228\nTest Accuracy (After fine-tuning): 0.98%\nTest F1 Score (After fine-tuning): 0.89\nTest Matthews Correlation Coefficient (After fine-tuning): 0.88\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 4, Model bert-base-multilingual-cased: 0\nTest Accuracy (Before fine-tuning): 0.38%\nTest F1 Score (Before fine-tuning): 0.16\nTest Matthews Correlation Coefficient (Before fine-tuning): 0.06\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 124/124 [00:42<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.2886\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 124/124 [00:42<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Loss: 0.0688\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 124/124 [00:42<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Loss: 0.0422\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 124/124 [00:42<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Loss: 0.0300\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 124/124 [00:42<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Loss: 0.0231\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 124/124 [00:42<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Loss: 0.0148\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 124/124 [00:42<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Loss: 0.0101\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 124/124 [00:42<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Loss: 0.0087\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 124/124 [00:42<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Loss: 0.0053\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 124/124 [00:42<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Loss: 0.0055\nTest Accuracy (After fine-tuning): 0.83%\nTest F1 Score (After fine-tuning): 0.46\nTest Matthews Correlation Coefficient (After fine-tuning): 0.48\nTraining and evaluation of model  bert-base-multilingual-cased  complete!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2de394472b9f49c88117a75526384fb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6178b921426e4a9a83b716460b4b8959"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f998bd81db746d68254172f02c87fd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"931c831838184f5e954e8c063712d655"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/542M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d96f384fff1a4e67909ff93acc0df0e6"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 1, Model distilbert-base-multilingual-cased: 1\nTest Accuracy (Before fine-tuning): 0.88%\nTest F1 Score (Before fine-tuning): 0.00\nTest Matthews Correlation Coefficient (Before fine-tuning): -0.03\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 123/123 [00:22<00:00,  5.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.3085\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 123/123 [00:22<00:00,  5.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Loss: 0.1430\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 123/123 [00:22<00:00,  5.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Loss: 0.1070\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 123/123 [00:22<00:00,  5.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Loss: 0.0947\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 123/123 [00:22<00:00,  5.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Loss: 0.0711\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 123/123 [00:22<00:00,  5.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Loss: 0.0490\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 123/123 [00:22<00:00,  5.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Loss: 0.0272\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 123/123 [00:22<00:00,  5.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Loss: 0.0137\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 123/123 [00:22<00:00,  5.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Loss: 0.0059\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 123/123 [00:22<00:00,  5.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Loss: 0.0052\nTest Accuracy (After fine-tuning): 0.96%\nTest F1 Score (After fine-tuning): 0.82\nTest Matthews Correlation Coefficient (After fine-tuning): 0.81\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 2, Model distilbert-base-multilingual-cased: 1\nTest Accuracy (Before fine-tuning): 0.10%\nTest F1 Score (Before fine-tuning): 0.18\nTest Matthews Correlation Coefficient (Before fine-tuning): 0.00\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 124/124 [00:22<00:00,  5.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.3259\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 124/124 [00:22<00:00,  5.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Loss: 0.1703\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 124/124 [00:22<00:00,  5.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Loss: 0.1291\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 124/124 [00:22<00:00,  5.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Loss: 0.1087\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 124/124 [00:22<00:00,  5.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Loss: 0.0947\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 124/124 [00:22<00:00,  5.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Loss: 0.0856\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 124/124 [00:22<00:00,  5.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Loss: 0.0637\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 124/124 [00:22<00:00,  5.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Loss: 0.0446\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 124/124 [00:22<00:00,  5.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Loss: 0.0233\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 124/124 [00:22<00:00,  5.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Loss: 0.0128\nTest Accuracy (After fine-tuning): 0.97%\nTest F1 Score (After fine-tuning): 0.83\nTest Matthews Correlation Coefficient (After fine-tuning): 0.82\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 3, Model distilbert-base-multilingual-cased: 1\nTest Accuracy (Before fine-tuning): 0.66%\nTest F1 Score (Before fine-tuning): 0.22\nTest Matthews Correlation Coefficient (Before fine-tuning): 0.10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 124/124 [00:22<00:00,  5.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.3164\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 124/124 [00:22<00:00,  5.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Loss: 0.1505\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 124/124 [00:22<00:00,  5.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Loss: 0.1082\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 124/124 [00:22<00:00,  5.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Loss: 0.0931\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 124/124 [00:22<00:00,  5.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Loss: 0.0668\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 124/124 [00:22<00:00,  5.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Loss: 0.0439\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 124/124 [00:22<00:00,  5.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Loss: 0.0334\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 124/124 [00:22<00:00,  5.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Loss: 0.0093\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 124/124 [00:22<00:00,  5.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Loss: 0.0070\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 124/124 [00:22<00:00,  5.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Loss: 0.0048\nTest Accuracy (After fine-tuning): 0.98%\nTest F1 Score (After fine-tuning): 0.88\nTest Matthews Correlation Coefficient (After fine-tuning): 0.87\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 4, Model distilbert-base-multilingual-cased: 1\nTest Accuracy (Before fine-tuning): 0.08%\nTest F1 Score (Before fine-tuning): 0.14\nTest Matthews Correlation Coefficient (Before fine-tuning): 0.00\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 124/124 [00:22<00:00,  5.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.3123\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 124/124 [00:22<00:00,  5.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Loss: 0.0617\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 124/124 [00:22<00:00,  5.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Loss: 0.0377\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 124/124 [00:22<00:00,  5.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Loss: 0.0322\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 124/124 [00:22<00:00,  5.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Loss: 0.0206\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 124/124 [00:22<00:00,  5.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Loss: 0.0153\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 124/124 [00:22<00:00,  5.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Loss: 0.0074\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 124/124 [00:22<00:00,  5.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Loss: 0.0034\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 124/124 [00:22<00:00,  5.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Loss: 0.0028\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 124/124 [00:22<00:00,  5.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Loss: 0.0009\nTest Accuracy (After fine-tuning): 0.83%\nTest F1 Score (After fine-tuning): 0.46\nTest Matthews Correlation Coefficient (After fine-tuning): 0.48\nTraining and evaluation of model  distilbert-base-multilingual-cased  complete!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb75bc08ed954cc698b8e24787dc18ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"615d391e28664537b2435105a6b22dfd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3334f0f3f74f411a9e1a37485914b6fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efe98e8587cd40ea915d50e1ef91e6a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df4733b7cb504454b62e84bbbc791f44"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 1, Model xlm-roberta-base: 2\nTest Accuracy (Before fine-tuning): 0.11%\nTest F1 Score (Before fine-tuning): 0.20\nTest Matthews Correlation Coefficient (Before fine-tuning): 0.00\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 123/123 [00:40<00:00,  3.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.3597\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 123/123 [00:40<00:00,  3.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Loss: 0.3106\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 123/123 [00:40<00:00,  3.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Loss: 0.3048\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 123/123 [00:40<00:00,  3.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Loss: 0.2125\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 123/123 [00:40<00:00,  3.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Loss: 0.1142\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 123/123 [00:40<00:00,  3.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Loss: 0.1068\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 123/123 [00:40<00:00,  3.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Loss: 0.0936\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 123/123 [00:40<00:00,  3.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Loss: 0.0831\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 123/123 [00:40<00:00,  3.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Loss: 0.0843\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 123/123 [00:40<00:00,  3.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Loss: 0.0622\nTest Accuracy (After fine-tuning): 0.97%\nTest F1 Score (After fine-tuning): 0.84\nTest Matthews Correlation Coefficient (After fine-tuning): 0.82\n","output_type":"stream"},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 2, Model xlm-roberta-base: 2\nTest Accuracy (Before fine-tuning): 0.90%\nTest F1 Score (Before fine-tuning): 0.00\nTest Matthews Correlation Coefficient (Before fine-tuning): 0.00\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 124/124 [00:40<00:00,  3.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.3138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 124/124 [00:40<00:00,  3.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Loss: 0.1460\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 124/124 [00:40<00:00,  3.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Loss: 0.1216\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 124/124 [00:40<00:00,  3.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Loss: 0.1139\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 124/124 [00:40<00:00,  3.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Loss: 0.0956\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 124/124 [00:40<00:00,  3.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Loss: 0.0849\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 124/124 [00:40<00:00,  3.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Loss: 0.0723\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 124/124 [00:40<00:00,  3.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Loss: 0.0527\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 124/124 [00:40<00:00,  3.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Loss: 0.0425\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 124/124 [00:40<00:00,  3.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Loss: 0.0254\nTest Accuracy (After fine-tuning): 0.95%\nTest F1 Score (After fine-tuning): 0.69\nTest Matthews Correlation Coefficient (After fine-tuning): 0.69\n","output_type":"stream"},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 3, Model xlm-roberta-base: 2\nTest Accuracy (Before fine-tuning): 0.90%\nTest F1 Score (Before fine-tuning): 0.00\nTest Matthews Correlation Coefficient (Before fine-tuning): 0.00\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 124/124 [00:40<00:00,  3.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.3263\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 124/124 [00:40<00:00,  3.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Loss: 0.1857\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 124/124 [00:40<00:00,  3.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Loss: 0.1226\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 124/124 [00:39<00:00,  3.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Loss: 0.1113\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 124/124 [00:40<00:00,  3.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Loss: 0.1083\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 124/124 [00:40<00:00,  3.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Loss: 0.1024\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 124/124 [00:40<00:00,  3.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Loss: 0.0984\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 124/124 [00:40<00:00,  3.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Loss: 0.0876\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 124/124 [00:40<00:00,  3.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Loss: 0.0849\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 124/124 [00:40<00:00,  3.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Loss: 0.0706\nTest Accuracy (After fine-tuning): 0.96%\nTest F1 Score (After fine-tuning): 0.77\nTest Matthews Correlation Coefficient (After fine-tuning): 0.76\n","output_type":"stream"},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 4, Model xlm-roberta-base: 2\nTest Accuracy (Before fine-tuning): 0.92%\nTest F1 Score (Before fine-tuning): 0.00\nTest Matthews Correlation Coefficient (Before fine-tuning): 0.00\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 124/124 [00:40<00:00,  3.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.3487\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 124/124 [00:40<00:00,  3.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Loss: 0.1610\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 124/124 [00:40<00:00,  3.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Loss: 0.0562\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 124/124 [00:40<00:00,  3.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Loss: 0.0527\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 124/124 [00:40<00:00,  3.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Loss: 0.0474\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 124/124 [00:40<00:00,  3.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Loss: 0.0453\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 124/124 [00:40<00:00,  3.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Loss: 0.0441\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 124/124 [00:40<00:00,  3.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Loss: 0.0404\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 124/124 [00:40<00:00,  3.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Loss: 0.0408\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 124/124 [00:40<00:00,  3.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Loss: 0.0395\nTest Accuracy (After fine-tuning): 0.81%\nTest F1 Score (After fine-tuning): 0.44\nTest Matthews Correlation Coefficient (After fine-tuning): 0.46\nTraining and evaluation of model  xlm-roberta-base  complete!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/453 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"152a926c4c154ec8939465e6c75388b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79e3e4d7f110405fac86406df2d172f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"662554bf3f184006adda67451482df4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/173 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4511facf924434ab23d44f75981ef05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/879 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a08c490129d405087e71c23428ce3a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"085f519d9a45420bad9b979d5a4ff8aa"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at timpal0l/mdeberta-v3-base-squad2 and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 1, Model timpal0l/mdeberta-v3-base-squad2: 3\nTest Accuracy (Before fine-tuning): 0.11%\nTest F1 Score (Before fine-tuning): 0.20\nTest Matthews Correlation Coefficient (Before fine-tuning): 0.00\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 123/123 [00:52<00:00,  2.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.2938\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 123/123 [00:52<00:00,  2.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Loss: 0.1436\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 123/123 [00:52<00:00,  2.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Loss: 0.1148\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 123/123 [00:52<00:00,  2.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Loss: 0.1103\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 123/123 [00:52<00:00,  2.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Loss: 0.0990\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 123/123 [00:52<00:00,  2.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Loss: 0.0974\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 123/123 [00:52<00:00,  2.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Loss: 0.0867\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 123/123 [00:52<00:00,  2.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Loss: 0.0730\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 123/123 [00:52<00:00,  2.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Loss: 0.0546\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 123/123 [00:52<00:00,  2.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Loss: 0.0436\nTest Accuracy (After fine-tuning): 0.98%\nTest F1 Score (After fine-tuning): 0.89\nTest Matthews Correlation Coefficient (After fine-tuning): 0.88\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at timpal0l/mdeberta-v3-base-squad2 and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 2, Model timpal0l/mdeberta-v3-base-squad2: 3\nTest Accuracy (Before fine-tuning): 0.90%\nTest F1 Score (Before fine-tuning): 0.00\nTest Matthews Correlation Coefficient (Before fine-tuning): 0.00\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 124/124 [00:52<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.2973\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 124/124 [00:52<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Loss: 0.1461\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 124/124 [00:52<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Loss: 0.1192\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 124/124 [00:52<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Loss: 0.1112\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 124/124 [00:52<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Loss: 0.1015\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 124/124 [00:52<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Loss: 0.0933\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 124/124 [00:52<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Loss: 0.0814\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 124/124 [00:52<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Loss: 0.0691\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 124/124 [00:52<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Loss: 0.0628\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 124/124 [00:52<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Loss: 0.0559\nTest Accuracy (After fine-tuning): 0.96%\nTest F1 Score (After fine-tuning): 0.77\nTest Matthews Correlation Coefficient (After fine-tuning): 0.76\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at timpal0l/mdeberta-v3-base-squad2 and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 3, Model timpal0l/mdeberta-v3-base-squad2: 3\nTest Accuracy (Before fine-tuning): 0.80%\nTest F1 Score (Before fine-tuning): 0.19\nTest Matthews Correlation Coefficient (Before fine-tuning): 0.09\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 124/124 [00:52<00:00,  2.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.3102\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 124/124 [00:52<00:00,  2.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Loss: 0.1339\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 124/124 [00:52<00:00,  2.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Loss: 0.1156\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 124/124 [00:52<00:00,  2.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Loss: 0.1080\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 124/124 [00:52<00:00,  2.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Loss: 0.1103\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 124/124 [00:52<00:00,  2.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Loss: 0.0958\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 124/124 [00:52<00:00,  2.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Loss: 0.0787\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 124/124 [00:52<00:00,  2.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Loss: 0.0707\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 124/124 [00:52<00:00,  2.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Loss: 0.0580\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 124/124 [00:52<00:00,  2.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Loss: 0.0407\nTest Accuracy (After fine-tuning): 0.97%\nTest F1 Score (After fine-tuning): 0.85\nTest Matthews Correlation Coefficient (After fine-tuning): 0.84\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at timpal0l/mdeberta-v3-base-squad2 and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 4, Model timpal0l/mdeberta-v3-base-squad2: 3\nTest Accuracy (Before fine-tuning): 0.49%\nTest F1 Score (Before fine-tuning): 0.09\nTest Matthews Correlation Coefficient (Before fine-tuning): -0.09\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 124/124 [00:52<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.3204\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 124/124 [00:52<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Loss: 0.1023\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 124/124 [00:52<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Loss: 0.0571\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 124/124 [00:52<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Loss: 0.0595\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 124/124 [00:52<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Loss: 0.0447\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 124/124 [00:52<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Loss: 0.0357\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 124/124 [00:52<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Loss: 0.0290\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 124/124 [00:52<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Loss: 0.0249\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 124/124 [00:52<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Loss: 0.0193\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 124/124 [00:52<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Loss: 0.0127\nTest Accuracy (After fine-tuning): 0.83%\nTest F1 Score (After fine-tuning): 0.46\nTest Matthews Correlation Coefficient (After fine-tuning): 0.48\nTraining and evaluation of model  timpal0l/mdeberta-v3-base-squad2  complete!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa3fdd497134467dbdd5619b8111befe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/430 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d36ae6edb4b4fd4b089c7dac545abbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c097477b25f24020954164fe282df8b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2f6779283b04e9e8a2e44dd659557b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f037193aac924180a69dbed3bc6dd5b7"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/Multilingual-MiniLM-L12-H384 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 1, Model microsoft/Multilingual-MiniLM-L12-H384: 4\nTest Accuracy (Before fine-tuning): 0.89%\nTest F1 Score (Before fine-tuning): 0.00\nTest Matthews Correlation Coefficient (Before fine-tuning): 0.00\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 123/123 [00:14<00:00,  8.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.3872\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 123/123 [00:14<00:00,  8.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Loss: 0.3071\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 123/123 [00:14<00:00,  8.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Loss: 0.2802\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 123/123 [00:14<00:00,  8.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Loss: 0.1623\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 123/123 [00:14<00:00,  8.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Loss: 0.1289\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 123/123 [00:14<00:00,  8.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Loss: 0.1231\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 123/123 [00:14<00:00,  8.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Loss: 0.1150\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 123/123 [00:14<00:00,  8.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Loss: 0.1086\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 123/123 [00:14<00:00,  8.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Loss: 0.1013\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 123/123 [00:14<00:00,  8.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Loss: 0.0973\nTest Accuracy (After fine-tuning): 0.97%\nTest F1 Score (After fine-tuning): 0.88\nTest Matthews Correlation Coefficient (After fine-tuning): 0.87\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/Multilingual-MiniLM-L12-H384 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 2, Model microsoft/Multilingual-MiniLM-L12-H384: 4\nTest Accuracy (Before fine-tuning): 0.90%\nTest F1 Score (Before fine-tuning): 0.00\nTest Matthews Correlation Coefficient (Before fine-tuning): 0.00\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 124/124 [00:14<00:00,  8.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.3908\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 124/124 [00:14<00:00,  8.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Loss: 0.3118\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 124/124 [00:14<00:00,  8.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Loss: 0.3100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 124/124 [00:14<00:00,  8.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Loss: 0.2737\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 124/124 [00:14<00:00,  8.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Loss: 0.1478\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 124/124 [00:14<00:00,  8.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Loss: 0.1307\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 124/124 [00:14<00:00,  8.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Loss: 0.1143\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 124/124 [00:14<00:00,  8.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Loss: 0.1088\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 124/124 [00:14<00:00,  8.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Loss: 0.1039\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 124/124 [00:14<00:00,  8.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Loss: 0.0963\nTest Accuracy (After fine-tuning): 0.97%\nTest F1 Score (After fine-tuning): 0.86\nTest Matthews Correlation Coefficient (After fine-tuning): 0.84\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/Multilingual-MiniLM-L12-H384 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 3, Model microsoft/Multilingual-MiniLM-L12-H384: 4\nTest Accuracy (Before fine-tuning): 0.10%\nTest F1 Score (Before fine-tuning): 0.18\nTest Matthews Correlation Coefficient (Before fine-tuning): 0.00\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 124/124 [00:14<00:00,  8.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.4017\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 124/124 [00:14<00:00,  8.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Loss: 0.3126\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 124/124 [00:14<00:00,  8.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Loss: 0.3122\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 124/124 [00:14<00:00,  8.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Loss: 0.3093\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 124/124 [00:14<00:00,  8.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Loss: 0.2744\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 124/124 [00:14<00:00,  8.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Loss: 0.1601\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 124/124 [00:14<00:00,  8.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Loss: 0.1464\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 124/124 [00:14<00:00,  8.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Loss: 0.1254\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 124/124 [00:14<00:00,  8.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Loss: 0.1171\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 124/124 [00:14<00:00,  8.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Loss: 0.1107\nTest Accuracy (After fine-tuning): 0.98%\nTest F1 Score (After fine-tuning): 0.91\nTest Matthews Correlation Coefficient (After fine-tuning): 0.90\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/Multilingual-MiniLM-L12-H384 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 4, Model microsoft/Multilingual-MiniLM-L12-H384: 4\nTest Accuracy (Before fine-tuning): 0.92%\nTest F1 Score (Before fine-tuning): 0.00\nTest Matthews Correlation Coefficient (Before fine-tuning): 0.00\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 124/124 [00:14<00:00,  8.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.4058\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 124/124 [00:14<00:00,  8.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Loss: 0.3286\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 124/124 [00:14<00:00,  8.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Loss: 0.3244\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 124/124 [00:14<00:00,  8.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Loss: 0.2324\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 124/124 [00:14<00:00,  8.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Loss: 0.0798\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 124/124 [00:14<00:00,  8.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Loss: 0.0572\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 124/124 [00:14<00:00,  8.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Loss: 0.0475\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 124/124 [00:14<00:00,  8.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Loss: 0.0450\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 124/124 [00:14<00:00,  8.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Loss: 0.0386\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 124/124 [00:14<00:00,  8.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Loss: 0.0356\nTest Accuracy (After fine-tuning): 0.83%\nTest F1 Score (After fine-tuning): 0.47\nTest Matthews Correlation Coefficient (After fine-tuning): 0.50\nTraining and evaluation of model  microsoft/Multilingual-MiniLM-L12-H384  complete!\nAverage Accuracy After Each Fold: 0.9324755493842067\nAverage F1 Score After Each Fold: 0.7291933387003103\nAverage Matthews Correlation Coefficient After Each Fold: 0.7285809654319604\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}